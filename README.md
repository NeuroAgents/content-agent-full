# AI Content Aggregator

Система для парсинга и агрегации статей по теме ИИ и ИИ-агентов из открытых источников с последующим сохранением в Supabase.

## Описание

Проект представляет собой инструмент для автоматического сбора статей по теме искусственного интеллекта из различных интернет-источников. Система поддерживает два типа источников:

- RSS-ленты (через библиотеку feedparser)
- HTML-страницы (через requests + BeautifulSoup)

Собранные статьи сохраняются в базу данных Supabase для дальнейшей обработки, перевода и публикации.

### Ключевые особенности

- Автоматическая загрузка полного контента статей (с использованием newspaper3k)
- Периодическое обновление источников через ежедневные задачи
- Возможность обновления контента существующих статей

## Структура проекта

```
├── fetch_articles.py     # Основной скрипт для загрузки статей
├── daily_update.py       # Скрипт для ежедневного обновления статей
├── update_content.py     # Скрипт для обновления полного контента существующих статей
├── check_content.py      # Скрипт для проверки длины контента статей
├── init_db.py            # Скрипт для инициализации базы данных
├── import_sources.py     # Скрипт для импорта источников из CSV/Excel
├── add_source.py         # Скрипт для добавления нового источника
├── .env                  # Файл с переменными окружения (не включен в репозиторий)
├── .env.example          # Пример файла с переменными окружения
├── requirements.txt      # Зависимости проекта
├── examples/             # Примеры файлов
│   ├── sources.csv       # Пример CSV с источниками
│   ├── sources.xlsx      # Пример Excel с источниками
├── logs/                 # Директория для файлов логов
│   ├── app.log           # Основной файл логов
├── supabase/             # Файлы для Supabase
│   ├── migrations/       # SQL-миграции
│       ├── 20231101000000_create_content_tables.sql  # Создание таблиц
├── src/
│   ├── parsers/          # Модули для парсинга источников
│   │   ├── base_parser.py   # Базовый класс парсера
│   │   ├── rss_parser.py    # Парсер RSS-лент (с загрузкой полного контента)
│   │   ├── html_parser.py   # Парсер HTML-страниц
│   ├── db/               # Модули для работы с базой данных
│   │   ├── supabase_client.py  # Клиент для Supabase
│   ├── utils/            # Вспомогательные утилиты
│   │   ├── logger.py        # Настройка логирования
```

## Установка и настройка

1. Клонируйте репозиторий:

```bash
git clone https://github.com/NeuroAgents/content-agent-full.git
cd content-agent-full
```

2. Установите зависимости:

```bash
pip install -r requirements.txt
```

3. Создайте файл `.env` на основе `.env.example` и укажите ваши параметры подключения к Supabase:

```
SUPABASE_URL=https://your-project-url.supabase.co
SUPABASE_KEY=your-supabase-key
LOG_LEVEL=INFO
LOG_FILE=logs/app.log
```

4. Инициализируйте базу данных:

```bash
python init_db.py
```

## Использование

### Загрузка статей

Для запуска процесса загрузки статей выполните:

```bash
python fetch_articles.py
```

### Ежедневное обновление статей

Для запуска ежедневного обновления статей (рекомендуется настроить через cron):

```bash
python daily_update.py
```

### Обновление контента существующих статей

Для загрузки полного контента существующих статей с коротким содержанием:

```bash
python update_content.py
```

### Проверка длины контента статей

Для проверки длины контента статей и выявления коротких статей:

```bash
python check_content.py
```

## Детальная документация

Более подробная документация доступна в ветке [feature/full-content-loading](https://github.com/NeuroAgents/content-agent-full/tree/feature/full-content-loading).

## Лицензия

Проект распространяется под лицензией MIT.
